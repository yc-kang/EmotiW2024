{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad8e9b2d-5638-444b-9353-69b51f29d0e0",
   "metadata": {
    "id": "ad8e9b2d-5638-444b-9353-69b51f29d0e0"
   },
   "source": [
    "### Video-LLaVa preprocessing\n",
    "\n",
    "Run via replicate API [[Example code](https://replicate.com/nateraw/video-llava/examples)]\n",
    "\n",
    "Runtime Experiments:\n",
    "- Current implementation, takes 10 mins per video\n",
    "- Run directly in replicate playground, takes 3 mins per video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a8b934-c4e8-4370-8d7a-eacdaf84a242",
   "metadata": {
    "id": "b1a8b934-c4e8-4370-8d7a-eacdaf84a242"
   },
   "source": [
    "### Experiment 1 (19 Nov)\n",
    "Prompt:\n",
    "- Only analyze the attention and engagement of the subject, provide 10 relevant keywords, seperation by ','\n",
    "\n",
    "Output Results (Sample):\n",
    "- Not-Engaged: Attention, engagement, headphones, music, focus, concentration, listening, relaxation, comfort, personal space, self-expression.\n",
    "- Barely-Engaged: Attention, engagement, glasses, white wall, white shirt, white background, white room, white surface, white background, white wall, white room.\n",
    "- Engaged: Attention, engagement, headphones, white, black, green, blue, white shirt, black shirt, white jacket, black jacket, white headband, black headband.\n",
    "- Highly-Engaged: Attention, engagement, headphones, woman, white, black, gray, blue, green, yellow, red, white."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install replicate"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pcWfyAJqBp0l",
    "outputId": "d241654c-1ec0-4e56-99cc-a4e63d166a19"
   },
   "id": "pcWfyAJqBp0l",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting replicate\n",
      "  Downloading replicate-1.0.3-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from replicate) (0.27.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from replicate) (24.2)\n",
      "Requirement already satisfied: pydantic>1.10.7 in /usr/local/lib/python3.10/dist-packages (from replicate) (2.9.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from replicate) (4.12.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (3.10)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.21.0->replicate) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>1.10.7->replicate) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>1.10.7->replicate) (2.23.4)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.21.0->replicate) (1.2.2)\n",
      "Downloading replicate-1.0.3-py3-none-any.whl (46 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m46.7/46.7 kB\u001B[0m \u001B[31m2.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: replicate\n",
      "Successfully installed replicate-1.0.3\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876b436a-197c-471b-ba5d-4384fe0c0cb1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "876b436a-197c-471b-ba5d-4384fe0c0cb1",
    "outputId": "81830cd6-e71c-489d-be2e-14d92c33a939"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Processing video: highly_engaged_0.mp4\n",
      "Finished processing video: highly_engaged_0.mp4. Output saved to highly_engaged_0_output.txt\n",
      "Processing video: not_engaged_0.mp4\n",
      "Finished processing video: not_engaged_0.mp4. Output saved to not_engaged_0_output.txt\n",
      "Processing video: barely_engaged_1.mp4\n",
      "Error processing video barely_engaged_1.mp4: [Errno 2] No such file or directory: '/content/drive/MyDrive/Video_test/barely_engaged_1.mp4'\n",
      "Processing video: not_engaged_2.mp4\n",
      "Finished processing video: not_engaged_2.mp4. Output saved to not_engaged_2_output.txt\n",
      "Processing video: engaged_0.mp4\n",
      "Error processing video engaged_0.mp4: [Errno 2] No such file or directory: '/content/drive/MyDrive/Video_test/engaged_0.mp4'\n",
      "Processing video: barely_engaged_2.mp4\n",
      "Error processing video barely_engaged_2.mp4: [Errno 2] No such file or directory: '/content/drive/MyDrive/Video_test/barely_engaged_2.mp4'\n",
      "Processing video: engaged_2.mp4\n",
      "Error processing video engaged_2.mp4: [Errno 2] No such file or directory: '/content/drive/MyDrive/Video_test/engaged_2.mp4'\n",
      "Processing video: highly_engaged_1.mp4\n",
      "Error processing video highly_engaged_1.mp4: [Errno 2] No such file or directory: '/content/drive/MyDrive/Video_test/highly_engaged_1.mp4'\n",
      "Processing video: barely_engaged_0.mp4\n",
      "Error processing video barely_engaged_0.mp4: [Errno 2] No such file or directory: '/content/drive/MyDrive/Video_test/barely_engaged_0.mp4'\n",
      "Processing video: engaged_1.mp4\n",
      "Error processing video engaged_1.mp4: [Errno 2] No such file or directory: '/content/drive/MyDrive/Video_test/engaged_1.mp4'\n",
      "Processing video: highly_engaged_2.mp4\n",
      "Error processing video highly_engaged_2.mp4: [Errno 2] No such file or directory: '/content/drive/MyDrive/Video_test/highly_engaged_2.mp4'\n",
      "Processing video: not_engaged_1.mp4\n",
      "Finished processing video: not_engaged_1.mp4. Output saved to not_engaged_1_output.txt\n",
      "Processing complete. Outputs saved in individual text files.\n"
     ]
    }
   ],
   "source": [
    "# 安装 replicate 模块\n",
    "!pip install replicate --quiet\n",
    "\n",
    "import os\n",
    "import replicate\n",
    "from google.colab import drive\n",
    "\n",
    "# 挂载 Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 设置 Replicate API token\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = \"\"  # API Token Here\n",
    "\n",
    "# 定义视频文件夹路径\n",
    "video_folder_path = \"/content/drive/MyDrive/Video_test\"\n",
    "\n",
    "# 定义分类提示文本\n",
    "text_prompt_string = \"\"\"\n",
    "Analyze the video and only analyze the main character. Provide a detailed description of the individuals present. Focus on the following aspects:\n",
    "\n",
    "Facial Micro-Expressions: Identify subtle facial movements, including smiles, frowns, eyebrow raises, eye squints, or lip movements. Note any indicators of emotions such as concentration, confusion, boredom, or interest.\n",
    "\n",
    "Eye Movement and Gaze: Determine the direction of each individual's gaze. Are they looking at a specific object, screen, another person, or elsewhere? Indicate whether their gaze suggests focus or distraction.\n",
    "\n",
    "Body Posture: Describe the posture of each person. Are they sitting upright, slouching, leaning forward attentively, or displaying any signs of physical restlessness (e.g., tapping fingers, shaking legs)?\n",
    "\n",
    "Hand and Arm Movements: Observe gestures such as pointing, writing, typing, or any fidgeting behaviors (e.g., playing with objects, scratching). Note if these actions contribute to or detract from their engagement.\n",
    "\n",
    "Head Movements: Analyze head tilts, nods, or shakes. Specify whether these movements indicate agreement, disagreement, curiosity, or disengagement.\n",
    "\n",
    "Overall Action and Behavior: Provide a summary of each individual's actions, including whether they are actively participating, passively observing, or distracted by unrelated activities.\n",
    "\n",
    "Ensure the analysis is precise and contextualized, capturing even slight variations in expressions, posture, and movements over time.\n",
    "\"\"\"\n",
    "\n",
    "# 遍历视频文件夹中的所有文件\n",
    "for video_name in os.listdir(video_folder_path):\n",
    "    video_path = os.path.join(video_folder_path, video_name)\n",
    "\n",
    "    # 检查文件是否为视频（根据扩展名过滤）\n",
    "    if video_name.lower().endswith(('.mp4', '.mov', '.avi', '.mkv')):\n",
    "        try:\n",
    "            print(f\"Processing video: {video_name}\")\n",
    "\n",
    "            # 打开视频文件\n",
    "            with open(video_path, \"rb\") as video_file:\n",
    "                # 定义输入参数\n",
    "                input = {\n",
    "                    \"video_path\": video_file,\n",
    "                    \"text_prompt\": text_prompt_string\n",
    "                }\n",
    "\n",
    "                # 调用 Replicate 模型\n",
    "                output = replicate.run(\n",
    "                    \"nateraw/video-llava:26387f81b9417278a8578188a31cd763eb3a55ca0f3ec375bf69c713de3fb4e8\",\n",
    "                    input=input\n",
    "                )\n",
    "\n",
    "                # 保存结果到单独的文本文件\n",
    "                output_file_name = f\"{os.path.splitext(video_name)[0]}_output.txt\"\n",
    "                output_file_path = os.path.join(video_folder_path, output_file_name)\n",
    "                with open(output_file_path, \"w\") as output_file:\n",
    "                    output_file.write(f\"Video: {video_name}\\n\")\n",
    "                    output_file.write(f\"Classification Output: {output}\\n\")\n",
    "                    output_file.write(\"=\"*50 + \"\\n\")\n",
    "\n",
    "            print(f\"Finished processing video: {video_name}. Output saved to {output_file_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {video_name}: {e}\")\n",
    "            # 保存错误信息到单独的文本文件\n",
    "            error_file_name = f\"{os.path.splitext(video_name)[0]}_error.txt\"\n",
    "            error_file_path = os.path.join(video_folder_path, error_file_name)\n",
    "            with open(error_file_path, \"w\") as error_file:\n",
    "                error_file.write(f\"Video: {video_name}\\n\")\n",
    "                error_file.write(f\"Error: {e}\\n\")\n",
    "                error_file.write(\"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Processing complete. Outputs saved in individual text files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96063b5e-1732-47fd-aecf-b8bcf799d97f",
   "metadata": {
    "id": "96063b5e-1732-47fd-aecf-b8bcf799d97f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "outputId": "1b8a22a8-d321-41c7-9b02-2f4ac6638099"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'video' is not defined",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-9aa76001a227>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m input = {\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0;34m\"video_path\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mvideo\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m     \u001B[0;34m\"text_prompt\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mtext_prompt_string\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m }\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'video' is not defined"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"video_path\": video,\n",
    "    \"text_prompt\": text_prompt_string\n",
    "}\n",
    "\n",
    "output = replicate.run(\n",
    "    \"nateraw/video-llava:26387f81b9417278a8578188a31cd763eb3a55ca0f3ec375bf69c713de3fb4e8\",\n",
    "    input=input\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4983cdb0-2972-4cb8-b832-6148cea89025",
   "metadata": {
    "id": "4983cdb0-2972-4cb8-b832-6148cea89025"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
